{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of final dataset\n",
    "\n",
    "We merge the information in ```df_tweets``` with the annotations.\n",
    "We extract the most common hashtags, mentions and emojis and turn them into binary variables (one-hot-encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_id</th>\n",
       "      <th>component_id</th>\n",
       "      <th>main_tweet</th>\n",
       "      <th>previous_context</th>\n",
       "      <th>posterior_context</th>\n",
       "      <th>Ad Hominem</th>\n",
       "      <th>Appeal to Fear</th>\n",
       "      <th>Appeal to Ridicule</th>\n",
       "      <th>False Dilemma</th>\n",
       "      <th>Hasty Generalization</th>\n",
       "      <th>...</th>\n",
       "      <th>created_at</th>\n",
       "      <th>followers</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144793</td>\n",
       "      <td>249</td>\n",
       "      <td>[user104337]: @user @user ... @user Kyrie Irv...</td>\n",
       "      <td>[user47446]: @user @user @user yeh bringing b...</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-06-13 19:03:08+00:00</td>\n",
       "      <td>252</td>\n",
       "      <td>92808</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>(nabinn_, TATAbox503, atantum99, BleacherReport)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124801</td>\n",
       "      <td>249</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally di...</td>\n",
       "      <td>[user47446]: @user @user @user yeh bringing b...</td>\n",
       "      <td>[user104337]: @user @user ... @user That's s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-06-13 19:38:45+00:00</td>\n",
       "      <td>386</td>\n",
       "      <td>6510</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>(ddpage369, nabinn_, TATAbox503, atantum99, Bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83279</td>\n",
       "      <td>249</td>\n",
       "      <td>[user104337]: @user @user ... @user That's so...</td>\n",
       "      <td>[user104337]: @user @user ... @user Kyrie Irv...</td>\n",
       "      <td>[user79987]: @user @user ... @user The unint...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-06-13 19:50:00+00:00</td>\n",
       "      <td>252</td>\n",
       "      <td>92808</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>(LuvLyricsQuotes, nabinn_, TATAbox503, atantum...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124800</td>\n",
       "      <td>249</td>\n",
       "      <td>[user79987]: @user @user ... @user The uninte...</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally di...</td>\n",
       "      <td>[user1779]: @user @user ... @user facts. if ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-06-13 20:13:49+00:00</td>\n",
       "      <td>386</td>\n",
       "      <td>6510</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>(ddpage369, nabinn_, TATAbox503, atantum99, Bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165415</td>\n",
       "      <td>249</td>\n",
       "      <td>[user47446]: @user @user ... @user It's been ...</td>\n",
       "      <td>[user79987]: @user @user ... @user The uninte...</td>\n",
       "      <td>[user1779]: @user @user ... @user this shit'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-06-13 21:21:31+00:00</td>\n",
       "      <td>127</td>\n",
       "      <td>4229</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>(wealljusteggsfr, LuvLyricsQuotes, ddpage369, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>152989</td>\n",
       "      <td>108897</td>\n",
       "      <td>[user16135]: @user just stop! Go grab a drink...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-04-27 00:35:17+00:00</td>\n",
       "      <td>83</td>\n",
       "      <td>6649</td>\n",
       "      <td>(WorstPresidentInHistory,)</td>\n",
       "      <td>()</td>\n",
       "      <td>(realDonaldTrump,)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>334394</td>\n",
       "      <td>118011</td>\n",
       "      <td>[user39585]: This is in reference to the @use...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-01-25 06:53:31+00:00</td>\n",
       "      <td>140</td>\n",
       "      <td>760</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>(WHO, NIH)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>194751</td>\n",
       "      <td>119026</td>\n",
       "      <td>[user20600]: Christina Cuomo Says She Took Cl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-04-25 16:54:28+00:00</td>\n",
       "      <td>2096</td>\n",
       "      <td>18165</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>269651</td>\n",
       "      <td>120326</td>\n",
       "      <td>[user23717]: Moderna To Seek Limited Emergenc...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-17 17:41:20+00:00</td>\n",
       "      <td>95</td>\n",
       "      <td>100176</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>277640</td>\n",
       "      <td>120904</td>\n",
       "      <td>[user68580]: NIH: Insufficient evidence on iv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-01-19 10:09:39+00:00</td>\n",
       "      <td>397</td>\n",
       "      <td>90084</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2916 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_id  component_id                                         main_tweet   \n",
       "0     144793           249   [user104337]: @user @user ... @user Kyrie Irv...  \\\n",
       "1     124801           249   [user79987]: @user @user ... @user Totally di...   \n",
       "2      83279           249   [user104337]: @user @user ... @user That's so...   \n",
       "3     124800           249   [user79987]: @user @user ... @user The uninte...   \n",
       "4     165415           249   [user47446]: @user @user ... @user It's been ...   \n",
       "...      ...           ...                                                ...   \n",
       "2911  152989        108897   [user16135]: @user just stop! Go grab a drink...   \n",
       "2912  334394        118011   [user39585]: This is in reference to the @use...   \n",
       "2913  194751        119026   [user20600]: Christina Cuomo Says She Took Cl...   \n",
       "2914  269651        120326   [user23717]: Moderna To Seek Limited Emergenc...   \n",
       "2915  277640        120904   [user68580]: NIH: Insufficient evidence on iv...   \n",
       "\n",
       "                                       previous_context   \n",
       "0      [user47446]: @user @user @user yeh bringing b...  \\\n",
       "1      [user47446]: @user @user @user yeh bringing b...   \n",
       "2      [user104337]: @user @user ... @user Kyrie Irv...   \n",
       "3      [user79987]: @user @user ... @user Totally di...   \n",
       "4      [user79987]: @user @user ... @user The uninte...   \n",
       "...                                                 ...   \n",
       "2911                                                      \n",
       "2912                                                      \n",
       "2913                                                      \n",
       "2914                                                      \n",
       "2915                                                      \n",
       "\n",
       "                                      posterior_context  Ad Hominem   \n",
       "0       [user79987]: @user @user ... @user Totally d...           0  \\\n",
       "1       [user104337]: @user @user ... @user That's s...           0   \n",
       "2       [user79987]: @user @user ... @user The unint...           0   \n",
       "3       [user1779]: @user @user ... @user facts. if ...           1   \n",
       "4       [user1779]: @user @user ... @user this shit'...           0   \n",
       "...                                                 ...         ...   \n",
       "2911                                                              1   \n",
       "2912                                                              0   \n",
       "2913                                                              0   \n",
       "2914                                                              0   \n",
       "2915                                                              0   \n",
       "\n",
       "      Appeal to Fear  Appeal to Ridicule  False Dilemma  Hasty Generalization   \n",
       "0                  0                   0              0                     0  \\\n",
       "1                  0                   0              0                     0   \n",
       "2                  0                   0              0                     0   \n",
       "3                  0                   0              0                     0   \n",
       "4                  0                   0              0                     0   \n",
       "...              ...                 ...            ...                   ...   \n",
       "2911               0                   1              0                     1   \n",
       "2912               1                   0              0                     1   \n",
       "2913               0                   0              0                     0   \n",
       "2914               0                   0              0                     0   \n",
       "2915               0                   0              0                     0   \n",
       "\n",
       "      ...                created_at  followers tweet_count   \n",
       "0     ... 2020-06-13 19:03:08+00:00        252       92808  \\\n",
       "1     ... 2020-06-13 19:38:45+00:00        386        6510   \n",
       "2     ... 2020-06-13 19:50:00+00:00        252       92808   \n",
       "3     ... 2020-06-13 20:13:49+00:00        386        6510   \n",
       "4     ... 2020-06-13 21:21:31+00:00        127        4229   \n",
       "...   ...                       ...        ...         ...   \n",
       "2911  ... 2020-04-27 00:35:17+00:00         83        6649   \n",
       "2912  ... 2021-01-25 06:53:31+00:00        140         760   \n",
       "2913  ... 2020-04-25 16:54:28+00:00       2096       18165   \n",
       "2914  ... 2020-09-17 17:41:20+00:00         95      100176   \n",
       "2915  ... 2021-01-19 10:09:39+00:00        397       90084   \n",
       "\n",
       "                        hashtags  cashtags   \n",
       "0                             ()        ()  \\\n",
       "1                             ()        ()   \n",
       "2                             ()        ()   \n",
       "3                             ()        ()   \n",
       "4                             ()        ()   \n",
       "...                          ...       ...   \n",
       "2911  (WorstPresidentInHistory,)        ()   \n",
       "2912                          ()        ()   \n",
       "2913                          ()        ()   \n",
       "2914                          ()        ()   \n",
       "2915                          ()        ()   \n",
       "\n",
       "                                               mentions retweet_count   \n",
       "0      (nabinn_, TATAbox503, atantum99, BleacherReport)             1  \\\n",
       "1     (ddpage369, nabinn_, TATAbox503, atantum99, Bl...             0   \n",
       "2     (LuvLyricsQuotes, nabinn_, TATAbox503, atantum...             0   \n",
       "3     (ddpage369, nabinn_, TATAbox503, atantum99, Bl...             0   \n",
       "4     (wealljusteggsfr, LuvLyricsQuotes, ddpage369, ...             0   \n",
       "...                                                 ...           ...   \n",
       "2911                                 (realDonaldTrump,)             0   \n",
       "2912                                         (WHO, NIH)             0   \n",
       "2913                                                 ()             0   \n",
       "2914                                                 ()             0   \n",
       "2915                                                 ()             0   \n",
       "\n",
       "     reply_count  like_count  quote_count  \n",
       "0              1           2            0  \n",
       "1              1           4            0  \n",
       "2              1           2            0  \n",
       "3              6           2            0  \n",
       "4              1           0            0  \n",
       "...          ...         ...          ...  \n",
       "2911           0           0            0  \n",
       "2912           0           0            0  \n",
       "2913           0           0            0  \n",
       "2914           0           0            0  \n",
       "2915           0           0            0  \n",
       "\n",
       "[2916 rows x 22 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sample information and annotations information\n",
    "df_tweets = pickle.load(open(\"results/df_tweets.pkl\", \"rb\"))\n",
    "df_multilabel_annotations = pd.read_csv(\"datasets/annotated_datasets/df_multilabel_annotations.csv\")\n",
    "df_multilabel_annotations['new_id'] = df_multilabel_annotations['new_id'].astype(str)\n",
    "\n",
    "# Left join the two dataframes\n",
    "df_multilabel_annotations = df_multilabel_annotations.merge(df_tweets, on=\"new_id\", how=\"left\", suffixes = (\"_x\", \"\")) \\\n",
    "\n",
    "# Keep only the relevant columns\n",
    "variables = ['new_id', 'component_id', 'main_tweet', 'previous_context', 'posterior_context', 'Ad Hominem', 'Appeal to Fear', 'Appeal to Ridicule', 'False Dilemma', 'Hasty Generalization', 'Loaded Language', 'None of the above', 'created_at', 'followers', 'tweet_count', 'hashtags', 'cashtags', 'mentions', 'retweet_count', 'reply_count', 'like_count', 'quote_count']\n",
    "df_multilabel_annotations = df_multilabel_annotations[variables]\n",
    "\n",
    "# Replace NaN values with empty strings in previous and posterior context\n",
    "df_multilabel_annotations['previous_context'] = df_multilabel_annotations['previous_context'].fillna('')\n",
    "df_multilabel_annotations['posterior_context'] = df_multilabel_annotations['posterior_context'].fillna('')\n",
    "\n",
    "# Remove [main_tweet] from the main tweet and [context] from previous and posterior context\n",
    "df_multilabel_annotations['main_tweet'] = df_multilabel_annotations['main_tweet'].apply(lambda x: re.sub(r'\\[main_tweet\\]', '', x))\n",
    "df_multilabel_annotations['previous_context'] = df_multilabel_annotations['previous_context'].apply(lambda x: re.sub(r'\\[context\\]', '', x))\n",
    "df_multilabel_annotations['posterior_context'] = df_multilabel_annotations['posterior_context'].apply(lambda x: re.sub(r'\\[context\\]', '', x))\n",
    "\n",
    "df_multilabel_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags, emojis and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_columns(df, column_name, elements_to_check):\n",
    "    '''\n",
    "    This function creates binary columns for each element in the list elements_to_check.\n",
    "\n",
    "    Args:\n",
    "    df: pandas DataFrame\n",
    "    column_name: str\n",
    "    elements_to_check: list\n",
    "\n",
    "    Returns:\n",
    "    df: pandas DataFrame\n",
    "    '''\n",
    "    new_columns = {}\n",
    "    for element in elements_to_check:\n",
    "        new_column_name = f'{column_name}_{element}'\n",
    "        new_columns[new_column_name] = df[column_name].apply(lambda x: 1 if element in x else 0)\n",
    "    df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "    return df\n",
    "\n",
    "def extract_emojis(text):\n",
    "    '''\n",
    "    Extracts emojis from a text, including compound emojis.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text from which to extract emojis.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the emojis found in the text.\n",
    "    '''\n",
    "    emoji_list = []\n",
    "\n",
    "    # Find all emojis using emoji's emojize function and regex\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        # Check for flags\n",
    "        if i+1<len(text) and re.match(r'[\\U0001F1E6-\\U0001F1FF]', text[i]) and re.match(r'[\\U0001F1E6-\\U0001F1FF]', text[i+1]):\n",
    "            emoji_list.append(text[i]+text[i+1])\n",
    "            i += 2\n",
    "\n",
    "        elif text[i] in emoji.EMOJI_DATA:\n",
    "            # Check for skin tone modifiers\n",
    "            if i+1<len(text) and re.match(r'[\\U0001F3FB-\\U0001F3FF]', text[i+1]):\n",
    "                if i+2<len(text) and text[i+2] in ['\\U00002642','\\U00002640','\\u2640', '\\u2642' ]:\n",
    "                    emoji_list.append(text[i]+text[i+1]+text[i+2])\n",
    "                    i += 3\n",
    "                emoji_list.append(text[i]+text[i+1])\n",
    "                i += 2\n",
    "            # Check for gender modifiers\n",
    "            elif i+1<len(text) and text[i+1] in ['\\U00002642','\\U00002640','\\u2640', '\\u2642' ]:\n",
    "                emoji_list.append(text[i]+text[i+1])\n",
    "                i += 2\n",
    "            else:\n",
    "                emoji_list.append(text[i])\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return tuple(emoji_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected hashtags, mentions and emojis\n",
    "with open('results/selected_elements.json', 'r') as json_file:\n",
    "    selected_elements = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_id</th>\n",
       "      <th>component_id</th>\n",
       "      <th>main_tweet</th>\n",
       "      <th>previous_context</th>\n",
       "      <th>posterior_context</th>\n",
       "      <th>Ad Hominem</th>\n",
       "      <th>Appeal to Fear</th>\n",
       "      <th>Appeal to Ridicule</th>\n",
       "      <th>False Dilemma</th>\n",
       "      <th>Hasty Generalization</th>\n",
       "      <th>...</th>\n",
       "      <th>emojis_‚§µ</th>\n",
       "      <th>emojis_üò∑</th>\n",
       "      <th>emojis_ü•≥</th>\n",
       "      <th>emojis_ü¶†</th>\n",
       "      <th>emojis_üò≠</th>\n",
       "      <th>emojis_ü§•</th>\n",
       "      <th>emojis_üòÅ</th>\n",
       "      <th>emojis_‚û°</th>\n",
       "      <th>emojis_üòâ</th>\n",
       "      <th>emojis_üá®üá¶</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144793</td>\n",
       "      <td>249</td>\n",
       "      <td>[user104337]: @user @user ... @user Kyrie Irv...</td>\n",
       "      <td>[user47446]: @user @user @user yeh bringing b...</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124801</td>\n",
       "      <td>249</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally di...</td>\n",
       "      <td>[user47446]: @user @user @user yeh bringing b...</td>\n",
       "      <td>[user104337]: @user @user ... @user That's s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83279</td>\n",
       "      <td>249</td>\n",
       "      <td>[user104337]: @user @user ... @user That's so...</td>\n",
       "      <td>[user104337]: @user @user ... @user Kyrie Irv...</td>\n",
       "      <td>[user79987]: @user @user ... @user The unint...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124800</td>\n",
       "      <td>249</td>\n",
       "      <td>[user79987]: @user @user ... @user The uninte...</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally di...</td>\n",
       "      <td>[user1779]: @user @user ... @user facts. if ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165415</td>\n",
       "      <td>249</td>\n",
       "      <td>[user47446]: @user @user ... @user It's been ...</td>\n",
       "      <td>[user79987]: @user @user ... @user The uninte...</td>\n",
       "      <td>[user1779]: @user @user ... @user this shit'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>152989</td>\n",
       "      <td>108897</td>\n",
       "      <td>[user16135]: @user just stop! Go grab a drink...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>334394</td>\n",
       "      <td>118011</td>\n",
       "      <td>[user39585]: This is in reference to the @use...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>194751</td>\n",
       "      <td>119026</td>\n",
       "      <td>[user20600]: Christina Cuomo Says She Took Cl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>269651</td>\n",
       "      <td>120326</td>\n",
       "      <td>[user23717]: Moderna To Seek Limited Emergenc...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>277640</td>\n",
       "      <td>120904</td>\n",
       "      <td>[user68580]: NIH: Insufficient evidence on iv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2916 rows √ó 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_id  component_id                                         main_tweet   \n",
       "0     144793           249   [user104337]: @user @user ... @user Kyrie Irv...  \\\n",
       "1     124801           249   [user79987]: @user @user ... @user Totally di...   \n",
       "2      83279           249   [user104337]: @user @user ... @user That's so...   \n",
       "3     124800           249   [user79987]: @user @user ... @user The uninte...   \n",
       "4     165415           249   [user47446]: @user @user ... @user It's been ...   \n",
       "...      ...           ...                                                ...   \n",
       "2911  152989        108897   [user16135]: @user just stop! Go grab a drink...   \n",
       "2912  334394        118011   [user39585]: This is in reference to the @use...   \n",
       "2913  194751        119026   [user20600]: Christina Cuomo Says She Took Cl...   \n",
       "2914  269651        120326   [user23717]: Moderna To Seek Limited Emergenc...   \n",
       "2915  277640        120904   [user68580]: NIH: Insufficient evidence on iv...   \n",
       "\n",
       "                                       previous_context   \n",
       "0      [user47446]: @user @user @user yeh bringing b...  \\\n",
       "1      [user47446]: @user @user @user yeh bringing b...   \n",
       "2      [user104337]: @user @user ... @user Kyrie Irv...   \n",
       "3      [user79987]: @user @user ... @user Totally di...   \n",
       "4      [user79987]: @user @user ... @user The uninte...   \n",
       "...                                                 ...   \n",
       "2911                                                      \n",
       "2912                                                      \n",
       "2913                                                      \n",
       "2914                                                      \n",
       "2915                                                      \n",
       "\n",
       "                                      posterior_context  Ad Hominem   \n",
       "0       [user79987]: @user @user ... @user Totally d...           0  \\\n",
       "1       [user104337]: @user @user ... @user That's s...           0   \n",
       "2       [user79987]: @user @user ... @user The unint...           0   \n",
       "3       [user1779]: @user @user ... @user facts. if ...           1   \n",
       "4       [user1779]: @user @user ... @user this shit'...           0   \n",
       "...                                                 ...         ...   \n",
       "2911                                                              1   \n",
       "2912                                                              0   \n",
       "2913                                                              0   \n",
       "2914                                                              0   \n",
       "2915                                                              0   \n",
       "\n",
       "      Appeal to Fear  Appeal to Ridicule  False Dilemma  Hasty Generalization   \n",
       "0                  0                   0              0                     0  \\\n",
       "1                  0                   0              0                     0   \n",
       "2                  0                   0              0                     0   \n",
       "3                  0                   0              0                     0   \n",
       "4                  0                   0              0                     0   \n",
       "...              ...                 ...            ...                   ...   \n",
       "2911               0                   1              0                     1   \n",
       "2912               1                   0              0                     1   \n",
       "2913               0                   0              0                     0   \n",
       "2914               0                   0              0                     0   \n",
       "2915               0                   0              0                     0   \n",
       "\n",
       "      ...  emojis_‚§µ  emojis_üò∑ emojis_ü•≥  emojis_ü¶†  emojis_üò≠ emojis_ü§• emojis_üòÅ   \n",
       "0     ...         0         0        0         0         0        0        0  \\\n",
       "1     ...         0         0        0         0         0        0        0   \n",
       "2     ...         0         0        0         0         0        0        0   \n",
       "3     ...         0         0        0         0         0        0        0   \n",
       "4     ...         0         0        0         0         0        0        0   \n",
       "...   ...       ...       ...      ...       ...       ...      ...      ...   \n",
       "2911  ...         0         0        0         0         0        0        0   \n",
       "2912  ...         0         0        0         0         0        0        0   \n",
       "2913  ...         0         0        0         0         0        0        0   \n",
       "2914  ...         0         0        0         0         0        0        0   \n",
       "2915  ...         0         0        0         0         0        0        0   \n",
       "\n",
       "     emojis_‚û°  emojis_üòâ  emojis_üá®üá¶  \n",
       "0           0         0          0  \n",
       "1           0         0          0  \n",
       "2           0         0          0  \n",
       "3           0         0          0  \n",
       "4           0         0          0  \n",
       "...       ...       ...        ...  \n",
       "2911        0         0          0  \n",
       "2912        0         0          0  \n",
       "2913        0         0          0  \n",
       "2914        0         0          0  \n",
       "2915        0         0          0  \n",
       "\n",
       "[2916 rows x 106 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract emojis from the main tweet\n",
    "df_multilabel_annotations['emojis'] = df_multilabel_annotations['main_tweet'].apply(extract_emojis)\n",
    "\n",
    "# Create binary columns for each selected element\n",
    "df_multilabel_annotations = create_binary_columns(df_multilabel_annotations, 'hashtags', selected_elements['selected_hashtags'])\n",
    "df_multilabel_annotations = create_binary_columns(df_multilabel_annotations, 'mentions', selected_elements['selected_mentions'])\n",
    "df_multilabel_annotations = create_binary_columns(df_multilabel_annotations, 'emojis', selected_elements['selected_emojis'])\n",
    "\n",
    "df_multilabel_annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER\n",
    "\n",
    "Hutto, C., & Gilbert, E. (2014). VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. Proceedings of the International AAAI Conference on Web and Social Media, 8(1), 216‚Äì225. https://doi.org/10.1609/icwsm.v8i1.14550\n",
    "\n",
    "Why this sentiment score? \n",
    "* Multidimentional (provides positive, negative, neutral, and compound).\n",
    "* Process raw text (it considers the effect of capital letters, punctuation, emojis...).\n",
    "* Performs exeptionally well in the social media domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "VADER_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_scores(text):\n",
    "    scores = VADER_analyzer.polarity_scores(text)\n",
    "    return pd.Series([scores['neg'], scores['neu'], scores['pos'], scores['compound']])\n",
    "\n",
    "df_multilabel_annotations[['VADER_neg', 'VADER_neu', 'VADER_pos', 'VADER_compound']] = df_multilabel_annotations['main_tweet'].apply(get_vader_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAD Lexicon\n",
    "\n",
    "Mohammad, S. M. (2018). Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words. Proceedings of The Annual Conference of the Association for Computational Linguistics (ACL).\n",
    "\n",
    "\n",
    "Why this score? \n",
    "\n",
    "* Multidimensional \n",
    "* Previously used for propaganda detection with good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /user/machaves/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /user/machaves/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAD lexicon\n",
    "vad_lexicon = {}\n",
    "with open('NRC-VAD-Lexicon/NRC-VAD-Lexicon.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t')\n",
    "        word = parts[0]\n",
    "        valence = float(parts[1])\n",
    "        arousal = float(parts[2])\n",
    "        dominance = float(parts[3])\n",
    "        vad_lexicon[word] = (valence, arousal, dominance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '[user1779]: @user @user ... @user none can readüòÇthat\\'s why i stopped responding. bro said \"how is it forgotten\" after i said \"it WILL BE forgotten\" comprehension is tough for them apparently so im done wasting my time. if they can\\'t see it\\'s gonna be a distraction from reality then ü§∑üèæ‚ôÇ \\n '\n",
    "\n",
    "def get_vad_lexicon_scores(text):\n",
    "    '''\n",
    "    This function calculates the average VAD scores for a text using the NRC VAD Lexicon.\n",
    "    It tokenizes and lowercases the text since the lexicon comprises lowercase words.\n",
    "    Lemmatization and stemming are not performed since the lexicon contains only base words.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate the VAD scores.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A pandas Series containing the average valence, arousal, and dominance scores for the text.\n",
    "    '''\n",
    "\n",
    "    # tokenize and lowercase the text\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # get the VAD scores for each word in the text\n",
    "    valence_scores = []\n",
    "    arousal_scores = []\n",
    "    dominance_scores = []\n",
    "    for word in words:\n",
    "        if word in vad_lexicon:\n",
    "            valence, arousal, dominance = vad_lexicon[word]\n",
    "            valence_scores.append(valence)\n",
    "            arousal_scores.append(arousal)\n",
    "            dominance_scores.append(dominance)\n",
    "\n",
    "    # get the average VAD scores for the text, or 0.5 if no words had scores (0.5 is the neutral score)\n",
    "    avg_valence = np.mean(valence_scores) if len(valence_scores) > 0 else 0.5\n",
    "    avg_arousal = np.mean(arousal_scores) if len(arousal_scores) > 0 else 0.5\n",
    "    avg_dominance = np.mean(dominance_scores) if len(dominance_scores) > 0 else 0.5\n",
    "\n",
    "    return pd.Series([avg_valence, avg_arousal, avg_dominance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multilabel_annotations[['VAD_valence', 'VAD_arousal', 'VAD_dominance']] = df_multilabel_annotations['main_tweet'].apply(get_vad_lexicon_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tags\n",
    "\n",
    "We experimented with POS tags generated by ```pos_tag``` from ```nltk```, ```textblob```, and ```spacy```.\n",
    "We observed better results with the later one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'JJ'), ('user101185', 'JJ'), (']', 'NN'), (':', ':'), ('Please', 'NNP'), ('DO', 'NNP'), ('NOT', 'NNP'), ('eat', 'VB'), ('or', 'CC'), ('drink', 'VB'), ('any', 'DT'), ('bleach', 'NN'), ('or', 'CC'), ('cleaners', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('NOT', 'VB'), ('clean', 'VB'), ('your', 'PRP$'), ('insides', 'NNS'), ('or', 'CC'), ('prevent', 'NN'), ('you', 'PRP'), ('from', 'IN'), ('getting', 'VBG'), ('#', '#'), ('COVID19', 'NNP'), ('I', 'PRP'), ('understand', 'VBP'), ('that', 'IN'), ('@', 'NNP'), ('user', 'NN'), ('has', 'VBZ'), ('suggested', 'VBN'), ('this', 'DT'), ('.', '.'), ('HE', 'NNP'), ('IS', 'VBZ'), ('WRONG', 'NNP'), ('.', '.'), ('Do', 'NNP'), ('Not', 'RB'), ('harm', 'VB'), ('yourself', 'PRP'), ('by', 'IN'), ('ingesting', 'VBG'), ('anything', 'NN'), ('but', 'CC'), ('food', 'NN'), ('and', 'CC'), ('beverage', 'NN'), ('.', '.'), ('ü§¶üèæ‚ôÄ', 'NN')]\n",
      "\n",
      "[('[', 'JJ'), ('user101185', 'JJ'), (']', 'NN'), ('Please', 'NNP'), ('DO', 'NNP'), ('NOT', 'NNP'), ('eat', 'VB'), ('or', 'CC'), ('drink', 'VB'), ('any', 'DT'), ('bleach', 'NN'), ('or', 'CC'), ('cleaners', 'NNS'), ('This', 'DT'), ('will', 'MD'), ('NOT', 'VB'), ('clean', 'VB'), ('your', 'PRP$'), ('insides', 'NNS'), ('or', 'CC'), ('prevent', 'NN'), ('you', 'PRP'), ('from', 'IN'), ('getting', 'VBG'), ('COVID19', 'NNP'), ('I', 'PRP'), ('understand', 'VBP'), ('that', 'IN'), ('@', 'NNP'), ('user', 'NN'), ('has', 'VBZ'), ('suggested', 'VBN'), ('this', 'DT'), ('HE', 'NNP'), ('IS', 'VBZ'), ('WRONG', 'NNP'), ('Do', 'NNP'), ('Not', 'RB'), ('harm', 'VB'), ('yourself', 'PRP'), ('by', 'IN'), ('ingesting', 'VBG'), ('anything', 'NN'), ('but', 'CC'), ('food', 'NN'), ('and', 'CC'), ('beverage', 'NN'), ('ü§¶üèæ‚ôÄ', 'NN')]\n",
      "\n",
      "[('[', 'X'), ('user101185', 'X'), (']', 'X'), (':', 'PUNCT'), ('Please', 'INTJ'), ('DO', 'AUX'), ('NOT', 'PART'), ('eat', 'VERB'), ('or', 'CCONJ'), ('drink', 'VERB'), ('any', 'DET'), ('bleach', 'NOUN'), ('or', 'CCONJ'), ('cleaners', 'NOUN'), ('.', 'PUNCT'), ('This', 'PRON'), ('will', 'AUX'), ('NOT', 'PART'), ('clean', 'VERB'), ('your', 'PRON'), ('insides', 'NOUN'), ('or', 'CCONJ'), ('prevent', 'VERB'), ('you', 'PRON'), ('from', 'ADP'), ('getting', 'VERB'), ('#', 'SYM'), ('COVID19', 'NOUN'), ('I', 'PRON'), ('understand', 'VERB'), ('that', 'SCONJ'), ('@user', 'PROPN'), ('has', 'AUX'), ('suggested', 'VERB'), ('this', 'PRON'), ('.', 'PUNCT'), ('HE', 'PRON'), ('IS', 'AUX'), ('WRONG', 'ADJ'), ('.', 'PUNCT'), ('Do', 'AUX'), ('Not', 'PART'), ('harm', 'VERB'), ('yourself', 'PRON'), ('by', 'ADP'), ('ingesting', 'VERB'), ('anything', 'PRON'), ('but', 'SCONJ'), ('food', 'NOUN'), (' ', 'SPACE'), ('and', 'CCONJ'), ('beverage', 'NOUN'), ('.', 'PUNCT'), ('ü§¶', 'NUM'), ('üèæ', 'PROPN'), ('‚ôÄ', 'PROPN'), ('\\n ', 'SPACE')]\n"
     ]
    }
   ],
   "source": [
    "sentence = '[user101185]: Please DO NOT eat or drink any bleach or cleaners. This will NOT clean your insides or prevent you from getting #COVID19 I understand that @user has suggested this. HE IS WRONG. Do Not harm yourself by ingesting anything but food  and beverage. ü§¶üèæ‚ôÄ \\n '\n",
    "\n",
    "# Example nltk pos_tag\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "wordtokens = word_tokenize(sentence)\n",
    "print(nltk.pos_tag(wordtokens),end='\\n\\n')\n",
    "\n",
    "# Example textblob\n",
    "from textblob import TextBlob\n",
    "text_blob = TextBlob(sentence)\n",
    "pos_tags = text_blob.tags\n",
    "print(pos_tags, end='\\n\\n')\n",
    "\n",
    "# Example spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sentence)\n",
    "print([(token.text, token.pos_)for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üíäThe', 'JJ'), ('drug', 'NN'), ('fostamatinib', 'NN'), ('could', 'MD'), ('be', 'VB'), ('repurposed', 'VBN'), ('to', 'TO'), ('treat', 'VB'), ('acute', 'JJ'), ('lung', 'NN'), ('injury', 'NN'), ('arising', 'VBG'), ('from', 'IN'), ('#', '#'), ('COVID19', 'NNP'), ('infection', 'NN'), (',', ','), ('says', 'VBZ'), ('a', 'DT'), ('preprint', 'NN'), ('featuring', 'VBG'), ('new', 'JJ'), ('research', 'NN'), ('led', 'VBN'), ('by', 'IN'), ('@', 'NNP'), ('user', 'NN'), (',', ','), ('with', 'IN'), ('@', 'NNP'), ('user', 'NN'), ('collaborators', 'NNS'), ('@', 'NNP'), ('user', 'NNP'), ('&', 'CC'), ('@', 'NNP'), ('user', 'VBP'), ('üîó', 'NNP'), ('Read', 'NNP'), ('the', 'DT'), ('preprint', 'NN'), (':', ':')]\n",
      "\n",
      "[('üíäThe', 'JJ'), ('drug', 'NN'), ('fostamatinib', 'NN'), ('could', 'MD'), ('be', 'VB'), ('repurposed', 'VBN'), ('to', 'TO'), ('treat', 'VB'), ('acute', 'JJ'), ('lung', 'NN'), ('injury', 'NN'), ('arising', 'VBG'), ('from', 'IN'), ('COVID19', 'NNP'), ('infection', 'NN'), ('says', 'VBZ'), ('a', 'DT'), ('preprint', 'NN'), ('featuring', 'VBG'), ('new', 'JJ'), ('research', 'NN'), ('led', 'VBN'), ('by', 'IN'), ('@', 'NNP'), ('user', 'NN'), ('with', 'IN'), ('@', 'NNP'), ('user', 'NN'), ('collaborators', 'NNS'), ('@', 'NNP'), ('user', 'NNP'), ('&', 'CC'), ('@', 'NNP'), ('user', 'VBP'), ('üîó', 'NNP'), ('Read', 'NNP'), ('the', 'DT'), ('preprint', 'NN')]\n",
      "\n",
      "[('üíä', 'NOUN'), ('The', 'DET'), ('drug', 'NOUN'), ('fostamatinib', 'NOUN'), ('could', 'AUX'), ('be', 'AUX'), ('repurposed', 'VERB'), ('to', 'PART'), ('treat', 'VERB'), ('acute', 'ADJ'), ('lung', 'NOUN'), ('injury', 'NOUN'), ('arising', 'VERB'), ('from', 'ADP'), ('#', 'SYM'), ('COVID19', 'NUM'), ('infection', 'NOUN'), (',', 'PUNCT'), ('says', 'VERB'), ('a', 'DET'), ('preprint', 'NOUN'), ('featuring', 'VERB'), ('new', 'ADJ'), ('research', 'NOUN'), ('led', 'VERB'), ('by', 'ADP'), ('@user', 'PROPN'), (',', 'PUNCT'), ('with', 'ADP'), ('@user', 'ADJ'), ('collaborators', 'NOUN'), ('@user', 'PROPN'), ('&', 'CCONJ'), ('@user', 'PROPN'), ('üîó', 'PROPN'), ('Read', 'VERB'), ('the', 'DET'), ('preprint', 'NOUN'), (':', 'PUNCT'), ('\\n ', 'SPACE')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'üíäThe drug fostamatinib could be repurposed to treat acute lung injury arising from #COVID19 infection, says a preprint featuring new research led by @user , with @user collaborators @user & @user üîó Read the preprint: \\n '\n",
    "\n",
    "# Example nltk pos_tag\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "wordtokens = word_tokenize(sentence)\n",
    "print(nltk.pos_tag(wordtokens),end='\\n\\n')\n",
    "\n",
    "# Example textblob\n",
    "from textblob import TextBlob\n",
    "text_blob = TextBlob(sentence)\n",
    "pos_tags = text_blob.tags\n",
    "print(pos_tags, end='\\n\\n')\n",
    "\n",
    "# Example spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sentence)\n",
    "print([(token.text, token.pos_)for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_id</th>\n",
       "      <th>component_id</th>\n",
       "      <th>main_tweet</th>\n",
       "      <th>previous_context</th>\n",
       "      <th>posterior_context</th>\n",
       "      <th>Ad Hominem</th>\n",
       "      <th>Appeal to Fear</th>\n",
       "      <th>Appeal to Ridicule</th>\n",
       "      <th>False Dilemma</th>\n",
       "      <th>Hasty Generalization</th>\n",
       "      <th>...</th>\n",
       "      <th>POS_NUM</th>\n",
       "      <th>POS_PART</th>\n",
       "      <th>POS_PRON</th>\n",
       "      <th>POS_PROPN</th>\n",
       "      <th>POS_PUNCT</th>\n",
       "      <th>POS_SCONJ</th>\n",
       "      <th>POS_SYM</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_X</th>\n",
       "      <th>POS_SPACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144793</td>\n",
       "      <td>249</td>\n",
       "      <td>[user104337]: @user @user ... @user Kyrie Irv...</td>\n",
       "      <td>[user47446]: @user @user @user yeh bringing b...</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124801</td>\n",
       "      <td>249</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally di...</td>\n",
       "      <td>[user47446]: @user @user @user yeh bringing b...</td>\n",
       "      <td>[user104337]: @user @user ... @user That's s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83279</td>\n",
       "      <td>249</td>\n",
       "      <td>[user104337]: @user @user ... @user That's so...</td>\n",
       "      <td>[user104337]: @user @user ... @user Kyrie Irv...</td>\n",
       "      <td>[user79987]: @user @user ... @user The unint...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124800</td>\n",
       "      <td>249</td>\n",
       "      <td>[user79987]: @user @user ... @user The uninte...</td>\n",
       "      <td>[user79987]: @user @user ... @user Totally di...</td>\n",
       "      <td>[user1779]: @user @user ... @user facts. if ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165415</td>\n",
       "      <td>249</td>\n",
       "      <td>[user47446]: @user @user ... @user It's been ...</td>\n",
       "      <td>[user79987]: @user @user ... @user The uninte...</td>\n",
       "      <td>[user1779]: @user @user ... @user this shit'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>152989</td>\n",
       "      <td>108897</td>\n",
       "      <td>[user16135]: @user just stop! Go grab a drink...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>334394</td>\n",
       "      <td>118011</td>\n",
       "      <td>[user39585]: This is in reference to the @use...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>194751</td>\n",
       "      <td>119026</td>\n",
       "      <td>[user20600]: Christina Cuomo Says She Took Cl...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>269651</td>\n",
       "      <td>120326</td>\n",
       "      <td>[user23717]: Moderna To Seek Limited Emergenc...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>277640</td>\n",
       "      <td>120904</td>\n",
       "      <td>[user68580]: NIH: Insufficient evidence on iv...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2916 rows √ó 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_id  component_id                                         main_tweet   \n",
       "0     144793           249   [user104337]: @user @user ... @user Kyrie Irv...  \\\n",
       "1     124801           249   [user79987]: @user @user ... @user Totally di...   \n",
       "2      83279           249   [user104337]: @user @user ... @user That's so...   \n",
       "3     124800           249   [user79987]: @user @user ... @user The uninte...   \n",
       "4     165415           249   [user47446]: @user @user ... @user It's been ...   \n",
       "...      ...           ...                                                ...   \n",
       "2911  152989        108897   [user16135]: @user just stop! Go grab a drink...   \n",
       "2912  334394        118011   [user39585]: This is in reference to the @use...   \n",
       "2913  194751        119026   [user20600]: Christina Cuomo Says She Took Cl...   \n",
       "2914  269651        120326   [user23717]: Moderna To Seek Limited Emergenc...   \n",
       "2915  277640        120904   [user68580]: NIH: Insufficient evidence on iv...   \n",
       "\n",
       "                                       previous_context   \n",
       "0      [user47446]: @user @user @user yeh bringing b...  \\\n",
       "1      [user47446]: @user @user @user yeh bringing b...   \n",
       "2      [user104337]: @user @user ... @user Kyrie Irv...   \n",
       "3      [user79987]: @user @user ... @user Totally di...   \n",
       "4      [user79987]: @user @user ... @user The uninte...   \n",
       "...                                                 ...   \n",
       "2911                                                      \n",
       "2912                                                      \n",
       "2913                                                      \n",
       "2914                                                      \n",
       "2915                                                      \n",
       "\n",
       "                                      posterior_context  Ad Hominem   \n",
       "0       [user79987]: @user @user ... @user Totally d...           0  \\\n",
       "1       [user104337]: @user @user ... @user That's s...           0   \n",
       "2       [user79987]: @user @user ... @user The unint...           0   \n",
       "3       [user1779]: @user @user ... @user facts. if ...           1   \n",
       "4       [user1779]: @user @user ... @user this shit'...           0   \n",
       "...                                                 ...         ...   \n",
       "2911                                                              1   \n",
       "2912                                                              0   \n",
       "2913                                                              0   \n",
       "2914                                                              0   \n",
       "2915                                                              0   \n",
       "\n",
       "      Appeal to Fear  Appeal to Ridicule  False Dilemma  Hasty Generalization   \n",
       "0                  0                   0              0                     0  \\\n",
       "1                  0                   0              0                     0   \n",
       "2                  0                   0              0                     0   \n",
       "3                  0                   0              0                     0   \n",
       "4                  0                   0              0                     0   \n",
       "...              ...                 ...            ...                   ...   \n",
       "2911               0                   1              0                     1   \n",
       "2912               1                   0              0                     1   \n",
       "2913               0                   0              0                     0   \n",
       "2914               0                   0              0                     0   \n",
       "2915               0                   0              0                     0   \n",
       "\n",
       "      ...  POS_NUM  POS_PART POS_PRON  POS_PROPN  POS_PUNCT POS_SCONJ POS_SYM   \n",
       "0     ...        0         4        8          4          9         1       0  \\\n",
       "1     ...        0         0        6          3         12         2       3   \n",
       "2     ...        0         2        7          4          6         0       0   \n",
       "3     ...        0         5        7          1          7         2       0   \n",
       "4     ...        2         3        9          3          7         1       0   \n",
       "...   ...      ...       ...      ...        ...        ...       ...     ...   \n",
       "2911  ...        0         0        4          3          4         1       0   \n",
       "2912  ...        1         1        6          6          7         0       0   \n",
       "2913  ...        0         0        1          7          1         0       0   \n",
       "2914  ...        0         1        0          8          1         1       0   \n",
       "2915  ...        1         0        0          4          4         0       1   \n",
       "\n",
       "     POS_VERB  POS_X  POS_SPACE  \n",
       "0           8      3          2  \n",
       "1           8      3          2  \n",
       "2           5      2          2  \n",
       "3          11      3          2  \n",
       "4           6      4          2  \n",
       "...       ...    ...        ...  \n",
       "2911        8      4          2  \n",
       "2912        4      3          2  \n",
       "2913        2      3          2  \n",
       "2914        2      2          2  \n",
       "2915        1      2          2  \n",
       "\n",
       "[2916 rows x 132 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def count_pos_tags(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # All possible POS tags\n",
    "    # I got them from the source code of spaCy: https://github.com/explosion/spaCy/blob/master/spacy/glossary.py\n",
    "    all_pos_tags = [ 'ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n",
    "\n",
    "    # Initialize dictionary with all possible POS tags set to 0\n",
    "    pos_counts = {tag: 0 for tag in all_pos_tags}\n",
    "\n",
    "    # Count each POS tag in the text\n",
    "    for token in doc:\n",
    "        pos_counts[token.pos_] += 1\n",
    "\n",
    "    return pos_counts\n",
    "\n",
    "# Apply count_pos_tags function to the 'text' column and expand the result into separate columns\n",
    "pos_counts_df = df_multilabel_annotations['main_tweet'].apply(count_pos_tags).apply(pd.Series)\n",
    "# Rename columns to add prefix 'POS_'\n",
    "pos_counts_df = pos_counts_df.add_prefix('POS_')\n",
    "\n",
    "# Concatenate the original DataFrame with the new POS counts DataFrame\n",
    "df_multilabel_annotations = pd.concat([df_multilabel_annotations, pos_counts_df], axis=1)\n",
    "\n",
    "df_multilabel_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validation, and test split\n",
    "\n",
    "We split the dataset in train, validation and test. The sampling is done considering the components, not individual tweets. Because of this, the classes can not be totally balanced. However we check that they are not too unbalanced in our split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split_by_groups(dataset, groups_variable, train_split, val_split, seed):\n",
    "    \"\"\"\n",
    "    Splits a dataset into training, validation, and test sets based on groups.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): The input dataset.\n",
    "    - groups_variable (str): The name of the variable containing group labels.\n",
    "    - train_split (float): The proportion of data to be included in the training set.\n",
    "    - val_split (float): The proportion of data to be included in the validation set.\n",
    "    - seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train (pd.DataFrame): Training set.\n",
    "    - val (pd.DataFrame): Validation set.\n",
    "    - test (pd.DataFrame): Test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get test split\n",
    "    test_split = 1 - train_split - val_split\n",
    "\n",
    "    # rows in dataset\n",
    "    n = len(dataset)\n",
    "\n",
    "    # get all groups\n",
    "    groups = np.unique(dataset[groups_variable])\n",
    "\n",
    "    # Generate random states for each split\n",
    "    np.random.seed(seed)\n",
    "    random1 = np.random.randint(0, 2**32)\n",
    "    random2 = np.random.randint(0, 2**32)\n",
    "\n",
    "    # Initialize sets for train, validation, and test groups\n",
    "    train_groups = set()\n",
    "    val_groups = set()\n",
    "    test_groups = set()\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    train_groups, test_groups = train_test_split(groups, test_size=test_split, random_state=random1)\n",
    "    if val_split > 0:\n",
    "        train_groups, val_groups = train_test_split(train_groups, test_size=val_split / (train_split + val_split),\n",
    "                                                   random_state=random2)\n",
    "\n",
    "    # Create subsets based on selected groups\n",
    "    train = dataset[dataset[groups_variable].isin(set(train_groups))]\n",
    "    val = dataset[dataset[groups_variable].isin(set(val_groups))]\n",
    "    test = dataset[dataset[groups_variable].isin(set(test_groups))]\n",
    "\n",
    "    # Print the percentage of rows in each set\n",
    "    print('% of rows in train', len(train) / n)\n",
    "    print('% of rows in validation', len(val) / n)\n",
    "    print('% of rows in test', len(test) / n)\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rows in train 0.6210562414266118\n",
      "% of rows in validation 0.1886145404663923\n",
      "% of rows in test 0.19032921810699588\n"
     ]
    }
   ],
   "source": [
    "# Get the train validation and test sets\n",
    "df_train, df_val , df_test = train_val_test_split_by_groups(dataset=df_multilabel_annotations, groups_variable='component_id', train_split=0.6, val_split=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of each fallacy in each dataset:\n",
      "                          train  validation       test\n",
      "Ad Hominem             9.166207    8.727273   8.108108\n",
      "Appeal to Fear         5.577029    4.909091   5.225225\n",
      "Appeal to Ridicule     8.117062    8.181818   8.288288\n",
      "False Dilemma          6.294865    4.727273   5.045045\n",
      "Hasty Generalization   3.036996    1.818182   4.684685\n",
      "Loaded Language       15.405853   15.636364  16.576577\n",
      "None of the above     64.494754   67.818182  65.945946\n"
     ]
    }
   ],
   "source": [
    "# Check how balanced the datasets are\n",
    "\n",
    "# Create a list of dataframes\n",
    "dataframes = {\"train\" : df_train, \"validation\" : df_val, \"test\" : df_test}\n",
    "\n",
    "# Create an empty dataframe to store the means\n",
    "fallacies = ['Ad Hominem', 'Appeal to Fear', 'Appeal to Ridicule', 'False Dilemma', 'Hasty Generalization', 'Loaded Language', 'None of the above']\n",
    "means_df = pd.DataFrame(index=fallacies)\n",
    "\n",
    "# Calculate means for each dataframe and store them in the means_df\n",
    "for name, df in dataframes.items():\n",
    "  # Calculate means for numeric columns (assuming binary variables are numeric 0/1)\n",
    "  means_df[name] = df[fallacies].mean(axis=0)*100\n",
    "\n",
    "# Create a crosstab with features as rows and dataframes as columns\n",
    "print('Percentage of each fallacy in each dataset:')\n",
    "print(means_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "df_train.to_csv(\"datasets/train_val_test_sets/df_train.csv\", index=False)\n",
    "df_val.to_csv(\"datasets/train_val_test_sets/df_val.csv\", index=False)\n",
    "df_test.to_csv(\"datasets/train_val_test_sets/df_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
