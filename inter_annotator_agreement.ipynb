{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from statsmodels.stats.proportion import proportions_chisquare_allpairs, proportions_ztest\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "# pd.options.display.max_columns=150\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_text_to_list(text):\n",
    "    \"\"\"\n",
    "    Parses annotation text and extracts a list of fallacies.\n",
    "\n",
    "    This function attempts to parse the input text as a dictionary (Example of text: {\"choices\":[\"Loaded Language\",\"Ad Hominem\"]}).\n",
    "    If successful and the dictionary contains a key named 'choices', it extracts the values associated with that key\n",
    "    and returns them as a list. If parsing fails or the 'choices' key is not found, the function\n",
    "    returns a list containing only the original text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The annotation text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of fallacies extracted from the annotation text. If parsing fails,\n",
    "              returns a list containing the original text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to parse the text as a dictionary\n",
    "        parsed = ast.literal_eval(text)\n",
    "        if isinstance(parsed, dict) and 'choices' in parsed:\n",
    "            # Extract values if 'choices' key exists\n",
    "            return parsed['choices']\n",
    "    except (SyntaxError, ValueError):\n",
    "        pass\n",
    "    # If not a dictionary or 'choices' key doesn't exist, return a list with the original text\n",
    "    return [text]\n",
    "\n",
    "def process_annotations_csv(csv_file):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes an annotations CSV file exported from Label Studio. Creates one-hot encoded columns in a dataframe. Each binary column corresponds to a label\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): The path to the CSV file containing annotation data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two data frames. The first data frame is the processed data with additional columns for each fallacy,\n",
    "               the second data frame contains the dummy variables created from the 'fallacy' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load annotations\n",
    "    data = pd.read_csv(csv_file)\n",
    "    if 'annotation_id' in data.columns:\n",
    "        data = data.drop(columns=['annotation_id'])\n",
    "    if 'created_at' in data.columns:\n",
    "        data = data.drop(columns=['created_at'])\n",
    "    data['new_id'] = data['new_id'].astype('string')\n",
    "\n",
    "    # Transform json like text to lists\n",
    "    data['fallacy'] = data['fallacy'].apply(annotation_text_to_list)\n",
    "\n",
    "    # Transform lists to dummy variables\n",
    "    dummy_variables = data['fallacy'].str.join('|').str.get_dummies()\n",
    "\n",
    "    # Move 'None of the above' to the last column on the dataset\n",
    "    col2 = dummy_variables.pop('None of the above')\n",
    "    position = dummy_variables.shape[1]\n",
    "    dummy_variables.insert(position, 'None of the above', col2)\n",
    "\n",
    "    # Add dummies to general data\n",
    "    data = pd.concat([data, dummy_variables], axis=1)\n",
    "\n",
    "    return data, dummy_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interannotator agreement\n",
    "\n",
    "Considering the following categorization of the Cohen's kappa *Hasty Generalization* shows only slight agreement; *False Dilemma* and *Loaded Language*, fair agreement; the other 4 categories show moderate agreement.  \n",
    "\n",
    "* < 0: No agreement (worse than chance)\n",
    "* 0.01-0.20: Slight agreement\n",
    "* 0.21-0.40: Fair agreement\n",
    "* 0.41-0.60: Moderate agreement\n",
    "* 0.61-0.80: Substantial agreement\n",
    "* 0.81-1.00: Almost perfect agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average kappa 0.666603705766611\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fallacy</th>\n",
       "      <th>kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appeal to Fear</td>\n",
       "      <td>0.805195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad Hominem</td>\n",
       "      <td>0.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appeal to Ridicule</td>\n",
       "      <td>0.767442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.724771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Loaded Language</td>\n",
       "      <td>0.562363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False Dilemma</td>\n",
       "      <td>0.554896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hasty Generalization</td>\n",
       "      <td>0.459459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fallacy     kappa\n",
       "1        Appeal to Fear  0.805195\n",
       "0            Ad Hominem  0.792100\n",
       "2    Appeal to Ridicule  0.767442\n",
       "6     None of the above  0.724771\n",
       "5       Loaded Language  0.562363\n",
       "3         False Dilemma  0.554896\n",
       "4  Hasty Generalization  0.459459"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load annotations for second round for second annotator and ressessment annotations\n",
    "df_annotations_multiple2_annotator2, dummy_variables = process_annotations_csv('datasets/annotated_datasets/annotations_multiple2_annotator2.csv')\n",
    "df_multilabel_annotations = pd.read_csv(\"datasets/annotated_datasets/df_multilabel_annotations.csv\")\n",
    "df_multilabel_annotations['new_id'] = df_multilabel_annotations['new_id'].astype(str)\n",
    "\n",
    "# Get names from dummy_variables\n",
    "fallacy_names = list(dummy_variables.columns)\n",
    "\n",
    "# Interannotator agreement by fallacy\n",
    "\n",
    "# Merge dataframes based on ID\n",
    "merged_df = pd.merge(df_multilabel_annotations[['new_id'] + fallacy_names],\n",
    "                     df_annotations_multiple2_annotator2[['new_id'] + fallacy_names],\n",
    "                     on='new_id', suffixes=('_1', '_2'), how='inner')\n",
    "\n",
    "kappa = []\n",
    "# Calculate Cohen's kappa for each fallacy\n",
    "for variable in fallacy_names:\n",
    "    annotator_1_labels = merged_df[variable + '_1']\n",
    "    annotator_2_labels = merged_df[variable + '_2']\n",
    "\n",
    "    kappa.append(cohen_kappa_score(annotator_1_labels, annotator_2_labels))\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "print('Average kappa', np.mean(kappa))\n",
    "df_kappa = pd.DataFrame({'fallacy': fallacy_names, 'kappa': kappa})\n",
    "df_kappa.sort_values(by='kappa', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
